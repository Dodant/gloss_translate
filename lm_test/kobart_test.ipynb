{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq,\n",
    "    BartForConditionalGeneration, BartTokenizer, pipeline\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mecab_ko as Mecab\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T06:30:10.840456626Z",
     "start_time": "2023-10-28T06:30:10.799270661Z"
    }
   },
   "id": "6e2db66e9cbb33cc"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-28T06:30:27.967109328Z",
     "start_time": "2023-10-28T06:30:11.874085553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/pytorch_model.bin from cache at /home/dodant/.cache/huggingface/transformers/163988ab4227d754ee71dba7a08d68888aca5394f672ed6957a4590cc1a38c56.146c3e1bbb2bd7a037d373ef715c53f6105bc7d00ad34e3304d597209b99226f\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at heegyu/kobart-text-style-transfer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/vocab.json from cache at /home/dodant/.cache/huggingface/transformers/b478971d39a8e846b0428e666000676068a67bb9975fb7bf17ed64b81f2f348d.a90b011e37fbb81820978fa316a49a85ea809362a79a5cd873f7c2531bedb6f8\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/merges.txt from cache at /home/dodant/.cache/huggingface/transformers/3b79939a026e9879fcc4714ded7025d45c57caf97ccb33c76973ac2c8d30b3c3.bef4d0a1ddc0882dc673f77f1562e08ddfd9e1e3604046a6493168b6b77c2c7e\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/added_tokens.json from cache at /home/dodant/.cache/huggingface/transformers/be6fc7b067c23a835d1a6919c409dc604ea980a7bdb7f6579342de28797da35b.04312f398a3bbda664297588800a86e0fda9d4ef4f0749cd9d96f88043daad39\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/special_tokens_map.json from cache at /home/dodant/.cache/huggingface/transformers/2c331b254b963e456432c79752463f4f17d62d49a793485de64636aa6de74c94.c23d5e62137984cf842a885705037b25b156747d145406702932d5f5d5e7c88e\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/tokenizer.json from cache at /home/dodant/.cache/huggingface/transformers/d2f310b398c4a7ab5869fcd2c4907687823a443eba54ee7063e3cd0603979341.dc2013f8bbecd755468e2c44397f53dc624be5451d0190744397caf61a20383f\n",
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"heegyu/kobart-text-style-transfer\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"heegyu/kobart-text-style-transfer\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"heegyu/kobart-text-style-transfer\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/pytorch_model.bin from cache at /home/dodant/.cache/huggingface/transformers/163988ab4227d754ee71dba7a08d68888aca5394f672ed6957a4590cc1a38c56.146c3e1bbb2bd7a037d373ef715c53f6105bc7d00ad34e3304d597209b99226f\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at heegyu/kobart-text-style-transfer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"heegyu/kobart-text-style-transfer\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/added_tokens.json from cache at /home/dodant/.cache/huggingface/transformers/be6fc7b067c23a835d1a6919c409dc604ea980a7bdb7f6579342de28797da35b.04312f398a3bbda664297588800a86e0fda9d4ef4f0749cd9d96f88043daad39\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/special_tokens_map.json from cache at /home/dodant/.cache/huggingface/transformers/2c331b254b963e456432c79752463f4f17d62d49a793485de64636aa6de74c94.c23d5e62137984cf842a885705037b25b156747d145406702932d5f5d5e7c88e\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/tokenizer.json from cache at /home/dodant/.cache/huggingface/transformers/d2f310b398c4a7ab5869fcd2c4907687823a443eba54ee7063e3cd0603979341.dc2013f8bbecd755468e2c44397f53dc624be5451d0190744397caf61a20383f\n",
      "loading configuration file https://huggingface.co/heegyu/kobart-text-style-transfer/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/779bf78efd6eb3e3487551bc1dde4b5ecae50902202a7c4ab92da7c04f204fc7.0e9e5b476887b939765fc5f36a4b627f02eb5e26ab7be309d95f3d7c30234f37\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"heegyu/kobart-text-style-transfer\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "같다 전공 친구. -> 문어체 -> 같은 학과 친구에요.\n",
      "같다 전공 친구. -> 구어체 -> 나는 같은 전공 친구야.\n",
      "같다 전공 친구. -> 안드로이드 -> 안드로이드. 전공. 친구.\n",
      "같다 전공 친구. -> 아재 -> 아 맞~ 전공 친구여~\n",
      "같다 전공 친구. -> 채팅 -> ᄋᄋ 난 전공 친구임\n",
      "같다 전공 친구. -> 초등학생 -> ᄋᄋ 같은 전공 친구임\n",
      "같다 전공 친구. -> 이모티콘 -> 난 전공 전공 친구야 ( ́`)\n",
      "같다 전공 친구. -> enfp -> 난 다 전공 친구야 ᄒᄒ\n",
      "같다 전공 친구. -> 신사 -> 같네 전공 친구입니다.\n",
      "같다 전공 친구. -> 할아버지 -> 나는... 전공이 같구먼...\n",
      "같다 전공 친구. -> 할머니 -> 염병 전공이여\n",
      "같다 전공 친구. -> 중학생 -> ᄋ 전공이 같음\n",
      "같다 전공 친구. -> 왕 -> 그렇소. 전공이 같소.\n",
      "같다 전공 친구. -> 나루토 -> 난 전공 전공이라니깐!\n",
      "같다 전공 친구. -> 선비 -> 소생은 전공이 같소!\n",
      "같다 전공 친구. -> 소심한 -> 난 전공 전공 친구야..\n",
      "같다 전공 친구. -> 번역기 -> 나는 같은 전공이 같다.\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"heegyu/kobart-text-style-transfer\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"heegyu/kobart-text-style-transfer\")\n",
    "\n",
    "class TextStyleTransferDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index, :]\n",
    "        text1 = row[0]\n",
    "        text2 = row[1]\n",
    "        target_style = row.index[1]\n",
    "        target_style_name = style_map[target_style]\n",
    "    \n",
    "        encoder_text = f\"{target_style_name} 말투로 변환: {text1}\"\n",
    "        decoder_text = f\"{text2}{self.tokenizer.eos_token}\"\n",
    "        model_inputs = self.tokenizer(encoder_text, max_length=64, truncation=True)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(decoder_text, max_length=64, truncation=True)\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        del model_inputs['token_type_ids']\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "def transfer_text_style(model, text, target_style, **kwargs):\n",
    "    input = f\"{target_style} 말투로 변환: {text}\"\n",
    "    out = model(input, max_length=64, **kwargs)\n",
    "    print(text, target_style, out[0]['generated_text'], sep=\" -> \")\n",
    "    \n",
    "model = pipeline('text2text-generation', model='heegyu/kobart-text-style-transfer')\n",
    "styles = ['문어체','구어체','안드로이드','아재','채팅', '초등학생','이모티콘','enfp','신사','할아버지','할머니','중학생', '왕','나루토','선비','소심한','번역기']\n",
    "style_map = {\n",
    "    'formal': '문어체',\n",
    "    'informal': '구어체',\n",
    "    'android': '안드로이드',\n",
    "    'azae': '아재',\n",
    "    'chat': '채팅',\n",
    "    'choding': '초등학생',\n",
    "    'emoticon': '이모티콘',\n",
    "    'enfp': 'enfp',\n",
    "    'gentle': '신사',\n",
    "    'halbae': '할아버지',\n",
    "    'halmae': '할머니',\n",
    "    'joongding': '중학생',\n",
    "    'king': '왕',\n",
    "    'naruto': '나루토',\n",
    "    'seonbi': '선비',\n",
    "    'sosim': '소심한',\n",
    "    'translator': '번역기'\n",
    "}\n",
    "\n",
    "text = \"같다 전공 친구.\"\n",
    "for style in styles: transfer_text_style(model, text, style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/pytorch_model.bin from cache at /home/dodant/.cache/huggingface/transformers/6a128677efa8d82c5bc9853bbefcff450bf4174bed52765687fc77f1aa7a39c1.ef5977990801f5b7dbc37adda9fe5948ed9829c75ac20e99bf026098743b1978\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/added_tokens.json from cache at /home/dodant/.cache/huggingface/transformers/7c75331e2f4b5767db997fbb489f1408eb36a3217beb3057ae8d04bd2b3f97ba.04312f398a3bbda664297588800a86e0fda9d4ef4f0749cd9d96f88043daad39\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/special_tokens_map.json from cache at /home/dodant/.cache/huggingface/transformers/a87d2ed77831bb40ce806a97c04126addf5ecc82b3e23ecf916b2a4acdb9c29a.c23d5e62137984cf842a885705037b25b156747d145406702932d5f5d5e7c88e\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/tokenizer.json from cache at /home/dodant/.cache/huggingface/transformers/f94202e1dad4fcfcb282aff4c6865b6119e03c87c6fa9e5886abe93835c41ecd.dc2013f8bbecd755468e2c44397f53dc624be5451d0190744397caf61a20383f\n",
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df = pd.read_csv(\"../MY_DATA/shots/gloss2text/0-shot.csv\")\n",
    "style_map = {'gloss': '글로스', 'spoken': '구어체'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:00:24.151059586Z",
     "start_time": "2023-10-28T07:00:19.410822194Z"
    }
   },
   "id": "655ef480dfba3a3f"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [14112, 11763, 12687, 14070, 13282, 10338, 14296, 13716, 257, 15015, 16687, 15851, 27583, 23925], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [15015, 14044, 15597, 15851, 14150, 15851, 14129, 11820, 12244, 262, 1]}\n",
      "[14112, 11763, 12687, 14070, 13282, 10338, 14296, 13716, 257, 15015, 16687, 15851, 27583, 23925]\n",
      "[15015, 14044, 15597, 15851, 14150, 15851, 14129, 11820, 12244, 262, 1]\n",
      "구어체 말투로 변환: 조선 시대 언제 까지?\n",
      "조선 시대는 언제부터 언제까지였죠?</s>\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"../MY_DATA/shots/gloss2text/0-shot.csv\")\n",
    "# style_map = {'gloss': '글로스', 'spoken': '구어체'}\n",
    "\n",
    "# df = pd.read_csv(\"../smilestyle_dataset.tsv\", sep=\"\\t\")\n",
    "# style_map = {\n",
    "#     'formal': '문어체',\n",
    "#     'informal': '구어체',\n",
    "#     'android': '안드로이드',\n",
    "#     'azae': '아재',\n",
    "#     'chat': '채팅',\n",
    "#     'choding': '초등학생',\n",
    "#     'emoticon': '이모티콘',\n",
    "#     'enfp': 'enfp',\n",
    "#     'gentle': '신사',\n",
    "#     'halbae': '할아버지',\n",
    "#     'halmae': '할머니',\n",
    "#     'joongding': '중학생',\n",
    "#     'king': '왕',\n",
    "#     'naruto': '나루토',\n",
    "#     'seonbi': '선비',\n",
    "#     'sosim': '소심한',\n",
    "#     'translator': '번역기'\n",
    "# }\n",
    "\n",
    "dataset = TextStyleTransferDataset(df, tokenizer)\n",
    "out = dataset[0]\n",
    "print(out)\n",
    "print(out['input_ids'])\n",
    "print(out['labels'])\n",
    "print(tokenizer.decode(out['input_ids']))\n",
    "print(tokenizer.decode(out['labels']))\n",
    "# out = dataset[1]\n",
    "# print(out['input_ids'])\n",
    "# print(out['labels'])\n",
    "# print(tokenizer.decode(out['input_ids']))\n",
    "# print(tokenizer.decode(out['labels']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:00:24.155766157Z",
     "start_time": "2023-10-28T07:00:24.152998503Z"
    }
   },
   "id": "53453a18816708dc"
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/config.json from cache at /home/dodant/.cache/huggingface/transformers/54a37e9385f90886428b084042f151c1a699203416d41765d94aac4cddb5fd5c.d098ef3866c1da94bdfaa5c1f24ecb7c5c16b37423b79263fbd3668d2ae61f91\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gogamza/kobart-base-v2/resolve/main/pytorch_model.bin from cache at /home/dodant/.cache/huggingface/transformers/6a128677efa8d82c5bc9853bbefcff450bf4174bed52765687fc77f1aa7a39c1.ef5977990801f5b7dbc37adda9fe5948ed9829c75ac20e99bf026098743b1978\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at gogamza/kobart-base-v2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.01, random_state=42)\n",
    "train_dataset, test_dataset = TextStyleTransferDataset(df_train, tokenizer), TextStyleTransferDataset(df_test, tokenizer)\n",
    "# print(len(df_train), len(df_test))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:00:26.960796321Z",
     "start_time": "2023-10-28T07:00:24.965712416Z"
    }
   },
   "id": "3f3161c064ed6eb4"
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../model_chp/bart-gloss2text3/runs/Oct30_23-44-56_dodant-ubt,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=30,\n",
      "output_dir=../model_chp/bart-gloss2text3,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../model_chp/bart-gloss2text3,\n",
      "save_on_each_node=False,\n",
      "save_steps=1000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=300,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_pth = \"../model_chp/bart-gloss2text3\"\n",
    "\n",
    "# from datasets import load_metric\n",
    "# \n",
    "# bleu_metric = load_metric(\"bleu\")\n",
    "# \n",
    "# def compute_metrics(p):\n",
    "#     decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=True) for pred in p.predictions]\n",
    "#     decoded_labels = [tokenizer.decode(label, skip_special_tokens=True, clean_up_tokenization_spaces=True) for label in p.label_ids]\n",
    "#     \n",
    "#     # BLEU 점수 계산\n",
    "#     bleu_score = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return {\"bleu\": bleu_score[\"score\"]}\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_pth, #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=30, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    eval_steps=500, # Number of update steps between two evaluations.\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    warmup_steps=300,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=3\n",
    ")\n",
    "print(training_args)\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     # compute_metrics=compute_metrics,\n",
    "# )\n",
    "# \n",
    "# trainer.train()\n",
    "# trainer.save_model(model_pth)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:44:56.824416219Z",
     "start_time": "2023-10-30T14:44:56.781607010Z"
    }
   },
   "id": "82fbbabaa18f5a4a"
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model_chp/bart-gloss2text3/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"../model_chp/bart-gloss2text3\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file ../model_chp/bart-gloss2text3/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"../model_chp/bart-gloss2text3\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file ../model_chp/bart-gloss2text3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ../model_chp/bart-gloss2text3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "nlg = pipeline('text2text-generation', model=model_pth, tokenizer=tokenizer)\n",
    "\n",
    "def generate_text(pipe, text, target_style, num_return_sequences=5, max_length=60):\n",
    "    target_style_name = style_map[target_style]\n",
    "    text = f\"{target_style_name} 말투로 변환: {text}\"\n",
    "    out = pipe(text, num_return_sequences=num_return_sequences, max_length=max_length)\n",
    "    return [x['generated_text'] for x in out]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:04:02.588340650Z",
     "start_time": "2023-10-28T07:04:01.798200370Z"
    }
   },
   "id": "e53af4b690643562"
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gloss', 'spoken'], dtype='object')\n",
      "입력 문장: \n",
      "어쩌다 마주친 그대 모습에 내 마음을 빼앗겨 버렸네\n",
      "어쩌다 마주친 그대 모습에 내 마음을 빼앗겼다.\n",
      "어쩌다 마주친 그대 모습에 내 마음을 빼앗겼다.\n"
     ]
    }
   ],
   "source": [
    "target_styles = df.columns\n",
    "print(target_styles)\n",
    "src_text = \"\"\"\n",
    "어쩌다 마주친 그대 모습에 내 마음을 빼앗겨 버렸네\n",
    "\"\"\"\n",
    "\n",
    "print(\"입력 문장:\", src_text)\n",
    "print(generate_text(nlg, src_text, 'gloss', num_return_sequences=1, max_length=1000)[0])\n",
    "print(generate_text(nlg, src_text, 'spoken', num_return_sequences=1, max_length=1000)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:04:05.567809695Z",
     "start_time": "2023-10-28T07:04:05.086557032Z"
    }
   },
   "id": "439a9bafce8a1c32"
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/128 안녕 ? 만나다 반갑다 . 나 농인 . -> 안녕하세요 만나서 반갑습니다.\n",
      " 안녕하십니까? 만나서 반갑습니다. 저는 농인입니다.\n",
      "BLEU score: 0.413 0.337 0.236 0.189 0.281\n",
      "1/128 안녕 ? 만나다 반갑다 . 나 청인 . -> 안녕하세요 만나서 반갑다\n",
      " 안녕하십니까? 만나서 반갑습니다. 저는 청인입니다.\n",
      "BLEU score: 0.263 0.184 0.074 0.000 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/128 당신 이름 무엇 ? -> 당신의 이름이 뭐에요?\n",
      " 당신의 이름은 무엇입니까?\n",
      "BLEU score: 0.500 0.286 0.167 0.000 0.000\n",
      "3/128 당신 농인 ? -> 당신은 농인인가요?\n",
      " 당신은 농인입니까?\n",
      "BLEU score: 0.800 0.500 0.333 0.000 0.000\n",
      "4/128 아니오 . 나 청인 . -> 나는 청인입니다.\n",
      " 아니오. 나는 청인입니다.\n",
      "BLEU score: 0.670 0.670 0.670 0.670 0.670\n",
      "5/128 고맙다 . 미안하다 . 괜찮다 . -> 고맙다\n",
      " 고맙습니다. 미안합니다. 괜찮습니다.\n",
      "BLEU score: 0.015 0.000 0.000 0.000 0.000\n",
      "6/128 수고 다음 또 만나다 -> 수고 다음에 또 만나자\n",
      " 수고하셨습니다. 다음에 또 뵙겠습니다.\n",
      "BLEU score: 0.245 0.147 0.092 0.000 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/128 나이 몇 ? 나 20 . -> 나이 몇 몇이죠?\n",
      " 나이가 어떻게 되세요? 나는 20살입니다.\n",
      "BLEU score: 0.123 0.000 0.000 0.000 0.000\n",
      "8/128 나 같다 . 나 20 . -> 나 같아 같아\n",
      " 저랑 동갑이군요. 나도 20살입니다.\n",
      "BLEU score: 0.049 0.000 0.000 0.000 0.000\n",
      "9/128 우리 둘 친하게 지내다 괜찮다 . -> 우리 둘 친하게 지내니까 괜찮아\n",
      " 우리 친하게 지냅시다.\n",
      "BLEU score: 0.375 0.143 0.000 0.000 0.000\n",
      "10/128 괜찮다 OK 좋다 . -> 괜찮아 OK, 좋아.\n",
      " 좋아요.\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "11/128 휴대폰 번호 무엇 ? -> 휴대폰 번 뭐에요?\n",
      " 휴대폰 번호가 어떻게 되나요?\n",
      "BLEU score: 0.282 0.000 0.000 0.000 0.000\n",
      "12/128 나 휴대폰 번호 010-2456-2642 . -> 내 휴대폰 번호 010-2456-2642.\n",
      " 내 휴대폰 번호는 010-2456-2642입니다.\n",
      "BLEU score: 0.801 0.601 0.458 0.267 0.492\n",
      "13/128 앞으로 자주 문자메시지를 주고받다 연락하다 OK . -> 앞으로 자주 문자메시지를 주고받기로 했는데 연락할게.\n",
      " 앞으로 자주 연락 나눠요.\n",
      "BLEU score: 0.357 0.154 0.083 0.000 0.000\n",
      "14/128 당신 1살 위 . -> 당신은 1살 위야\n",
      " 저보다 한 살 더 많으시네요.\n",
      "BLEU score: 0.101 0.000 0.000 0.000 0.000\n",
      "15/128 나 2살 아래 . -> 나는 2살 아래야\n",
      " 제가 두 살 더 적습니다.\n",
      "BLEU score: 0.119 0.000 0.000 0.000 0.000\n",
      "16/128 그 신발 돈 얼마 ? -> 그 신발 돈 얼마야?\n",
      " 그 신발은 얼마입니까?\n",
      "BLEU score: 0.667 0.200 0.000 0.000 0.000\n",
      "17/128 4만 5천 . -> 4만 5천\n",
      " 4만 5천원입니다.\n",
      "BLEU score: 0.472 0.472 0.472 0.472 0.472\n",
      "18/128 같다 전공 친구 . -> 같은 전공 친구야.\n",
      " 같은 과 친구예요.\n",
      "BLEU score: 0.667 0.200 0.000 0.000 0.000\n",
      "19/128 당신 전공 무엇 ? -> 당신의 전공 무엇이에요?\n",
      " 전공이 무엇인가요?\n",
      "BLEU score: 0.571 0.000 0.000 0.000 0.000\n",
      "20/128 나 전공 무엇 수화 통역 . -> 내 전공은 뭐 수화 통역야.\n",
      " 나는 수화통역을 전공하고 있습니다.\n",
      "BLEU score: 0.344 0.098 0.000 0.000 0.000\n",
      "21/128 가족 소개 주세요 . -> 가족 소개 소개해주세요.\n",
      " 가족 소개를 해 주세요.\n",
      "BLEU score: 0.857 0.667 0.400 0.250 0.489\n",
      "22/128 나 가족 부모님 오빠 나 4 사람 . -> 내 가족 부모님 오빠는 4 사람.\n",
      " 우리 가족은 부모님 오빠 나 모두 4명입니다.\n",
      "BLEU score: 0.478 0.179 0.102 0.000 0.000\n",
      "23/128 나 가족 조부모님 부모님 누나 나 남동생 7사람 . -> 내 가족 조부모님 부모님 누나. 남동생 7명, 나 남동생 7명.\n",
      " 우리 가족은 할아버지 할머니 부모님 누나 나 남동생 모두 7명입니다.\n",
      "BLEU score: 0.529 0.250 0.067 0.000 0.000\n",
      "24/128 가족 많다 부럽다 . -> 가족들이 많으니 부럽다.\n",
      " 가족이 많아 부럽군요.\n",
      "BLEU score: 0.625 0.143 0.000 0.000 0.000\n",
      "25/128 나 전공 사회 행복 . -> 나는 전공 사회에서의 행복해.\n",
      " 내 전공은 사회복지입니다.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "26/128 나 특별 교육 공부 . -> 나는 특별 교육 공부야.\n",
      " 내 전공은 특수교육입니다.\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "27/128 저 사람 같다 전공 선배 . -> 저 사람 같아 전공 선배야.\n",
      " 저분은 같은 과 선배입니다.\n",
      "BLEU score: 0.441 0.000 0.000 0.000 0.000\n",
      "28/128 저 사람 동아리 동기 . -> 저 사람 동아리 동기야.\n",
      " 저 사람은 동아리 동기입니다.\n",
      "BLEU score: 0.705 0.339 0.000 0.000 0.000\n",
      "29/128 당신 결혼 끝 ? -> 당신 결혼 결혼 끝인가요?\n",
      " 당신은 결혼을 했습니까?\n",
      "BLEU score: 0.423 0.000 0.000 0.000 0.000\n",
      "30/128 아니오 . 결혼 아직 . -> 결혼식이 아직 안 했어요.\n",
      " 아니오. 아직 안 했습니다.\n",
      "BLEU score: 0.571 0.333 0.200 0.000 0.000\n",
      "31/128 나 태어나다 곳 부산 . -> 나는 태어난는 태어난 곳 부산이다.\n",
      " 내 고향은 부산입니다.\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "32/128 부모님 태어나다 곳 부산 같다 ? -> 부모님 태어난 태어난 곳이 부산이신 것 같아요?\n",
      " 부모님 고향도 부산입니까?\n",
      "BLEU score: 0.308 0.083 0.000 0.000 0.000\n",
      "33/128 아니오 . 아버지 대구 어머니 부산 . -> 아버지는 대구 대구의 어머니였어요.\n",
      " 아니오. 아버지 고향은 대구이고 어머니 고향은 부산입니다.\n",
      "BLEU score: 0.255 0.000 0.000 0.000 0.000\n",
      "34/128 나 태어나다 곳 천안 . -> 나는 태어난는 곳의 천안\n",
      " 내 고향은 천안입니다.\n",
      "BLEU score: 0.143 0.000 0.000 0.000 0.000\n",
      "35/128 나 태어나다 곳 천안 호두 과자 유명하다 . -> 나는 태어난는 곳의 천안 호두 과자가 유명해.\n",
      " 내 고향 천안은 호두과자가 유명합니다.\n",
      "BLEU score: 0.308 0.083 0.000 0.000 0.000\n",
      "36/128 춘천 닭 갈비 유명하다 전주 비빔밥 유명하다 . -> 춘천의 닭 갈비가 유명하다.\n",
      " 춘천에는 닭갈비가 유명하고 전주는 비빔밥이 우명합니다.\n",
      "BLEU score: 0.285 0.128 0.073 0.000 0.000\n",
      "37/128 우리 둘 함께 전국 맛있다 음식 찾다 여행 출발 어때 ? -> 우리는 둘 함께 전국 맛있는 음식을 찾으러 여행 출발했다.\n",
      " 우리 함께 전국의 맛있는 음식을 찾아 여행을 떠날까요?\n",
      "BLEU score: 0.562 0.333 0.214 0.154 0.280\n",
      "38/128 보성 방문 경험 ? -> 보성 방문 경험했어?\n",
      " 보성에 가본 적 있나요?\n",
      "BLEU score: 0.239 0.000 0.000 0.000 0.000\n",
      "39/128 녹차밭 예쁘다 . -> 녹차밭이 너무 예쁘다.\n",
      " 녹차 밭이 무척 예뻤어요.\n",
      "BLEU score: 0.571 0.333 0.200 0.000 0.000\n",
      "40/128 당신 미국 방문 경험 ? -> 당신은 미국 방문 경험 있어요?\n",
      " 미국에 가본 적 있어요?\n",
      "BLEU score: 0.500 0.286 0.167 0.000 0.000\n",
      "41/128 그러나 1 방문 원하다 . -> 그러나 1 방문하기를 원해요.\n",
      " 하지만 꼭 한번 가보고 싶어요.\n",
      "BLEU score: 0.110 0.000 0.000 0.000 0.000\n",
      "42/128 좋다 음식 무엇 ? -> 좋은 음식은 뭐에요?\n",
      " 좋아하는 음식이 무엇인가요?\n",
      "BLEU score: 0.331 0.000 0.000 0.000 0.000\n",
      "43/128 나 비빔밥 좋다 . -> 나는 비빔밥이 좋아.\n",
      " 나는 비빔밥을 좋아해요.\n",
      "BLEU score: 0.743 0.433 0.173 0.000 0.000\n",
      "44/128 비빔밥 맛있다 이루다 장소 알다 ? -> 비빔밥을 맛있게 이루는 장소를 알아?\n",
      " 비빔밥 맛있는 음식점을 알고 있나요?\n",
      "BLEU score: 0.545 0.000 0.000 0.000 0.000\n",
      "45/128 먹다 장소 유명하다 . -> 먹는 장 장소가 유명해.\n",
      " 식당이 유명해요.\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "46/128 당신 취미 무엇 ? -> 당신의 취미 뭐에요?\n",
      " 취미가 뭐예요?\n",
      "BLEU score: 0.429 0.000 0.000 0.000 0.000\n",
      "47/128 나 취미 무엇 영화 보다 . -> 나는 취미로 뭐 영화를 본다.\n",
      " 내 취미는 영화감상이에요.\n",
      "BLEU score: 0.444 0.000 0.000 0.000 0.000\n",
      "48/128 요즈음 재미있다 영화 무엇 ? -> 요즈음 가장 재미있었던 영화 뭐야?\n",
      " 요즈음 재미있는 영화가 있나요?\n",
      "BLEU score: 0.444 0.000 0.000 0.000 0.000\n",
      "49/128 그러나 한국 영화 자막 없다 섭섭하다 . -> 그러나 한국 영화 자막이 없어서 섭섭해.\n",
      " 하지만 한국 영화는 자막이 없어서 안타까워요.\n",
      "BLEU score: 0.633 0.402 0.226 0.129 0.294\n",
      "50/128 배고프다 . 밥 출발 . -> 배고프다 밥 출발했어.\n",
      " 배고파요. 밥 먹으러 갑시다.\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "51/128 나 같다 . 먹다 무엇 ? -> 나 같아 같아\n",
      " 저도요. 뭐 먹으러 갈까요?\n",
      "BLEU score: 0.000 0.000 0.000 0.000 0.000\n",
      "52/128 맵다 김치 끓이다 괜찮다 ? -> 매운 김치 끓여야 하나요?\n",
      " 얼큰한 김치찌개 어때요?\n",
      "BLEU score: 0.167 0.000 0.000 0.000 0.000\n",
      "53/128 나 맵다 싫다 . -> 나 맵기 싫어\n",
      " 나는 매운 음식을 싫어해요.\n",
      "BLEU score: 0.270 0.112 0.000 0.000 0.000\n",
      "54/128 좋아하다 운동 무엇 ? -> 좋아하는 운동 운동 뭐에요?\n",
      " 좋아하는 운동이 뭐예요?\n",
      "BLEU score: 0.700 0.444 0.375 0.286 0.427\n",
      "55/128 수영 탁구 좋아하다 . -> 수영 탁구를 좋아해\n",
      " 나는 수영과 탁구를 좋아해요.\n",
      "BLEU score: 0.167 0.074 0.000 0.000 0.000\n",
      "56/128 당신 취미 무엇 ? -> 당신의 취미 뭐에요?\n",
      " 취미가 무엇인가요?\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "57/128 나 취미 독서 음악 듣다 . -> 나는 취미로 독서 음악을 듣는다.\n",
      " 내 취미는 독서와 음악감상이에요.\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "58/128 4올 20일 날 무엇 ? -> 4올 20일은 뭐야?\n",
      " 4월 20일이 무슨 날인가요?\n",
      "BLEU score: 0.441 0.126 0.000 0.000 0.000\n",
      "59/128 장애인 날 . -> 장애인 날입니다.\n",
      " 장애인의 날이에요.\n",
      "BLEU score: 0.327 0.000 0.000 0.000 0.000\n",
      "60/128 그렇군요 ! 농인 날 언제 ? -> 농인 날이 언제죠?\n",
      " 그렇군요! 농인의 날은 언제인가요?\n",
      "BLEU score: 0.342 0.000 0.000 0.000 0.000\n",
      "61/128 6월 3일 . -> 6월 3일은\n",
      " 6월 3일이에요.\n",
      "BLEU score: 0.536 0.503 0.447 0.335 0.448\n",
      "62/128 10시 30분 . -> 10시 30분이야\n",
      " 10시 30분이에요.\n",
      "BLEU score: 0.705 0.677 0.635 0.564 0.643\n",
      "63/128 11시 공부 시작 . 나 먼저 가다 . -> 11시부터 공부 시작하자.\n",
      " 11시에 수업이 시작합니다. 나 먼저 가볼게요.\n",
      "BLEU score: 0.268 0.076 0.000 0.000 0.000\n",
      "64/128 알다 . 다음 또 만나다 . -> 다음에 또 만날게\n",
      " 알았어요. 다음에 또 만나요.\n",
      "BLEU score: 0.221 0.184 0.123 0.000 0.000\n",
      "65/128 당신 태어나다 날 언제 ? -> 당신이 태어난어진 날은 언제인가요?\n",
      " 생일이 언제인가요?\n",
      "BLEU score: 0.444 0.250 0.143 0.000 0.000\n",
      "66/128 8월 18일 . -> 8월 18일\n",
      " 8월 18일이에요.\n",
      "BLEU score: 0.472 0.472 0.472 0.472 0.472\n",
      "67/128 5월 15일 날 무엇 ? -> 5월 15일 날은 뭐에요?\n",
      " 5월 15일은 무슨 날인가요?\n",
      "BLEU score: 0.700 0.333 0.250 0.143 0.302\n",
      "68/128 교사 날 . -> 교사는 날이다.\n",
      " 스승의 날이에요.\n",
      "BLEU score: 0.500 0.200 0.000 0.000 0.000\n",
      "69/128 7시 시작 . -> 7시가 시작됐어\n",
      " 7시에 시작합니다.\n",
      "BLEU score: 0.500 0.200 0.000 0.000 0.000\n",
      "70/128 공부 시작 9시 끝 12시 . -> 공부 시작 9시 끝 12시야.\n",
      " 수업은9시에 시작해서 12시에 끝납니다.\n",
      "BLEU score: 0.478 0.179 0.000 0.000 0.000\n",
      "71/128 오늘 봄 비 . -> 오늘은 봄 비가 왔다.\n",
      " 오늘은 봄비가 내리고 있어요.\n",
      "BLEU score: 0.441 0.126 0.000 0.000 0.000\n",
      "72/128 비 멈추다 날씨 따뜻하다 아마 . -> 비가 멈추니 날씨가 따뜻해.\n",
      " 비가 그치면 날씨가 따뜻해질 겁니다.\n",
      "BLEU score: 0.597 0.336 0.128 0.000 0.000\n",
      "73/128 며칠 기다리다 꽃피다 오다 아마 ? -> 며칠만 기다려서 꽃피다 올까?\n",
      " 며칠 있으면 꽃이 피겠군요?\n",
      "BLEU score: 0.215 0.000 0.000 0.000 0.000\n",
      "74/128 나 봄 노랗다 꽃 좋아하다 . -> 나는 봄 노랗고 꽃들을 좋아해.\n",
      " 나는 봄에 피는 개나리를 좋아합니다.\n",
      "BLEU score: 0.327 0.182 0.102 0.000 0.000\n",
      "75/128 좋다 계절 무엇 ? -> 좋은 계절소는 뭐에요?\n",
      " 어떤 계절을 좋아하나요?\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "76/128 나 가을 좋다 하늘 깨끗하다 빨갛다 잎 멋있다 . -> 나는 가을에 좋은 하늘에서 빨갛다 잎을 멋있게 봤어.\n",
      " 맑은 하늘과 단풍이 멋진 가을이 좋습니다.\n",
      "BLEU score: 0.294 0.062 0.000 0.000 0.000\n",
      "77/128 나 눈 겨울 좋다 . -> 내 눈 겨울이야.\n",
      " 나는 눈 내리는 겨울이 좋습니다.\n",
      "BLEU score: 0.342 0.103 0.000 0.000 0.000\n",
      "78/128 한국 계절 각각 멋있다 모두 좋다 . -> 한국의 계절은 각각 멋있어서 모두 좋다.\n",
      " 우리나라의 사철은 나름대로 운치 있어 모두 좋습니다.\n",
      "BLEU score: 0.346 0.076 0.000 0.000 0.000\n",
      "79/128 하늘 구름 비 아마 . -> 하늘의 구름은 비를 뿌려\n",
      " 하늘이 흐린 걸 보니 비가 올 모양이에요.\n",
      "BLEU score: 0.121 0.000 0.000 0.000 0.000\n",
      "80/128 우산 미리 준비 부탁 . -> 우산 미리 준비 부탁해.\n",
      " 우산을 미리 준비하세요.\n",
      "BLEU score: 0.564 0.169 0.000 0.000 0.000\n",
      "81/128 날씨 춥다 옷 옷이 두껍다 입다 부탁 . -> 날씨가 춥다, 옷이 두꺼운 거 입어요.\n",
      " 날씨가 추우니까 옷을 두둑하게 입으세요.\n",
      "BLEU score: 0.417 0.091 0.000 0.000 0.000\n",
      "82/128 가다 어느 곳 ? -> 가는 어느 곳이야?\n",
      " 지금 어디에 가요?\n",
      "BLEU score: 0.143 0.000 0.000 0.000 0.000\n",
      "83/128 책 읽다 집 가다 중 . -> 책을 읽다가 집에 가는 중이야.\n",
      " 도서관에 가고 있어요.\n",
      "BLEU score: 0.250 0.091 0.000 0.000 0.000\n",
      "84/128 책 읽다 집 가다 왜 ? -> 책을 읽다가 집에 갔어?\n",
      " 도서관에는 무슨 일로 가나요?\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "85/128 다음주 시험 때문에 공부 위하여 . -> 다음주 시험 때문에 공부해야 돼요.\n",
      " 다음 주가 시험이라 공부하러 갑니다.\n",
      "BLEU score: 0.398 0.000 0.000 0.000 0.000\n",
      "86/128 당신 일 무엇 ? -> 당신 일이 뭐에요?\n",
      " 무슨 일 하세요?\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "87/128 나 농인 협회 수화 통역 사람 . -> 나는 농인 협회 수화 통역 사람야.\n",
      " 농아인협회에서 수화통역사로 일합니다.\n",
      "BLEU score: 0.298 0.000 0.000 0.000 0.000\n",
      "88/128 수화 통역 사람 일 무엇 ? -> 수화 통역하는 사람 일은 무엇이에요?\n",
      " 수화통역사는 무슨 일을 하나요?\n",
      "BLEU score: 0.455 0.000 0.000 0.000 0.000\n",
      "89/128 농인 청인 대화 돕다 사람 . -> 농인과 청인 간의 대화를 도와준다.\n",
      " 농인과 청인의 의사소통을 지원합니다.\n",
      "BLEU score: 0.500 0.222 0.125 0.000 0.000\n",
      "90/128 숙제 끝 ? -> 숙제 다 했어?\n",
      " 과제 다 했니?\n",
      "BLEU score: 0.600 0.250 0.000 0.000 0.000\n",
      "91/128 숙제 무엇 ? 아이쿠 잊다 . -> 숙제 뭐야?\n",
      " 과제라니? 아이쿠. 깜빡 잊고 있었어.\n",
      "BLEU score: 0.034 0.000 0.000 0.000 0.000\n",
      "92/128 미래 원하다 희망 무엇 ? -> 미래에 대한 대한 희망은 뭐에요?\n",
      " 장래희망이 무엇인가요?\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "93/128 나 유치원 교사 원하다 . -> 나는 유치원 교사를 원해.\n",
      " 나는 유치원선생님이 되고 싶어요.\n",
      "BLEU score: 0.323 0.188 0.113 0.000 0.000\n",
      "94/128 나 아기 좋아하다 . -> 나는 아기를 좋아해\n",
      " 어린이들을 좋아하거든요.\n",
      "BLEU score: 0.000 0.000 0.000 0.000 0.000\n",
      "95/128 당신 잘 어울리다 . -> 당신에게 잘 어울려.\n",
      " 당신에게 잘 어울리는 거 같아요.\n",
      "BLEU score: 0.359 0.225 0.150 0.000 0.000\n",
      "96/128 내일 여자 친구 생일 . -> 내일은 여자 친구의 생일이에요\n",
      " 내일이 여자친구 생일이에요.\n",
      "BLEU score: 0.750 0.429 0.167 0.000 0.000\n",
      "97/128 선물 준비 끝 ? -> 선물 준비 준비 다 했어?\n",
      " 선물은 준비했나요?\n",
      "BLEU score: 0.571 0.000 0.000 0.000 0.000\n",
      "98/128 준비 아직 . 좋은 선물 무엇 ? -> 준비했 있었어?\n",
      " 아직 준비를 못했어요. 뭐가 좋을까요?\n",
      "BLEU score: 0.184 0.000 0.000 0.000 0.000\n",
      "99/128 꽃 또 화장품 어때 ? -> 꽃 또 화장품 어때?\n",
      " 꽃과 화장품은 어때요?\n",
      "BLEU score: 0.491 0.000 0.000 0.000 0.000\n",
      "100/128 동생 살다 잘 ? -> 동생이 잘 키워줬어?\n",
      " 동생은 잘 지내고 있나요?\n",
      "BLEU score: 0.372 0.000 0.000 0.000 0.000\n",
      "101/128 진동 시계 선물 주다 해보다 ? -> 진동 시계 선물 주려면 해볼까?\n",
      " 진동알람시계를 선물해 주는 건 어떄요?\n",
      "BLEU score: 0.455 0.000 0.000 0.000 0.000\n",
      "102/128 원하다 물건 무엇 ? -> 원하는 물건 물건 뭐에요?\n",
      " 찾으시는 물건이 있나요?\n",
      "BLEU score: 0.333 0.125 0.000 0.000 0.000\n",
      "103/128 우선 구경 보다 . -> 우선 구경 보자\n",
      " 우선 둘러볼게요.\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "104/128 이건 구두 마음에 들다 . 270 주세요. -> 이건 구발이 마음에 들어. 270 주세요.\n",
      " 이 구두가 마음에 들어요. 270사이즈 주세요.\n",
      "BLEU score: 0.690 0.418 0.184 0.000 0.000\n",
      "105/128 미안 270 바닥나다 ? -> 미안 27 270은 바닥이야?\n",
      " 죄송합니다. 270사이즈는 다 팔리고 없습니다.\n",
      "BLEU score: 0.076 0.000 0.000 0.000 0.000\n",
      "106/128 천안 부터 대전 까지 가장 빠르다 방법 무엇 ? -> 천안 부터 대전까지 가장 빠를 수 있는 방법은 뭐에요?\n",
      " 천안에서 대전까지 가는 가장 빠른 방법은 무엇인가요?\n",
      "BLEU score: 0.533 0.143 0.000 0.000 0.000\n",
      "107/128 KTX 비싸 ? -> KTX는 비싼가요?\n",
      " KTX는 비싸지 않나요?\n",
      "BLEU score: 0.402 0.168 0.000 0.000 0.000\n",
      "108/128 농인 50% 할인 도움 받다 조금 괜찮다 . -> 농인 50%의 도움을 받았으니 조금 괜찮아요.\n",
      " 농인은 50% 할인이 되기 때문에 부담이 덜합니다.\n",
      "BLEU score: 0.264 0.071 0.000 0.000 0.000\n",
      "109/128 여름 방학 계획 무엇 ? -> 여름 방학 계획이 뭐에요?\n",
      " 이번 여름방학 계획이 어떻게 되나요?\n",
      "BLEU score: 0.552 0.378 0.294 0.176 0.323\n",
      "110/128 친구 함께 자전거 여행 결정 . -> 친구와 함께 자전거 여행을 결정했다.\n",
      " 친구들과 함께 자전거 여행을 하기로 했어요.\n",
      "BLEU score: 0.519 0.247 0.185 0.106 0.224\n",
      "111/128 여행 목적 어느 곳 ? -> 여행 목적 어느 곳이에요?\n",
      " 어디로 가나요?\n",
      "BLEU score: 0.143 0.000 0.000 0.000 0.000\n",
      "112/128 서울 출발 부산 까지 . -> 서울에서 출발 부산까지예요.\n",
      " 서울에서 출발하여 부산까지 갑니다.\n",
      "BLEU score: 0.644 0.376 0.150 0.000 0.000\n",
      "113/128 운전 가능 ? -> 운전할 수 있어?\n",
      " 운전할 수 있어요?\n",
      "BLEU score: 0.833 0.600 0.500 0.333 0.537\n",
      "114/128 아직 배우다 중 그러나 주차 가장 어렵다 . -> 아직 배우는 않은 중 그러나 주차는 가장 어렵다.\n",
      " 아니요. 지금 배우고 있어요. 그런데 주차가 가장 어려운 것 같아요.\n",
      "BLEU score: 0.226 0.000 0.000 0.000 0.000\n",
      "115/128 나 같다 -> 나 같아 같아\n",
      " 나도 그랬어요.\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "116/128 합격 꼭 화이팅 . -> 합격 꼭 꼭 화이팅!\n",
      " 꼭 합격하시길 바랍니다.\n",
      "BLEU score: 0.268 0.000 0.000 0.000 0.000\n",
      "117/128 소화 안돼 머리 아프다 . -> 소화 안돼 머리가 아프다.\n",
      " 소화가 잘 안 되고 머리가 아파요.\n",
      "BLEU score: 0.487 0.111 0.000 0.000 0.000\n",
      "118/128 병원 가다 오다 끝 ? -> 병원에 데려 오라고 했어?\n",
      " 병원에는 다녀왔어요?\n",
      "BLEU score: 0.375 0.143 0.000 0.000 0.000\n",
      "119/128 2시 수화 통역 사람 만나다 약속 . -> 2시 수화 통역 사람과 만날 약속이 있어요.\n",
      " 2시에 수화통역사와 만나기로 했어요.\n",
      "BLEU score: 0.417 0.182 0.000 0.000 0.000\n",
      "120/128 오늘 공부 쉬다 . -> 오늘 공부 쉬어요.\n",
      " 이번 수업 휴강이래요.\n",
      "BLEU score: 0.164 0.000 0.000 0.000 0.000\n",
      "121/128 왜 ? -> 왜??\n",
      " 왜요 ?\n",
      "BLEU score: 0.667 0.000 0.000 0.000 0.000\n",
      "122/128 교사 맹장 수술 때문에 입원 중 . -> 교사가 맹장 수술 때문에 입원 중이야.\n",
      " 교수님께서 맹장수술을 하고 입원해 계신대요.\n",
      "BLEU score: 0.332 0.091 0.000 0.000 0.000\n",
      "123/128 함께 병 위로 출발 . -> 함께 병 위로 출발할게요.\n",
      " 우리 함께 병문안 갑시다.\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "124/128 어제 치아 아프다 데굴데굴 구르다 잠 못 이루다 . -> 어제 치아 아프다 데굴데굴 구르느라 잠을 못 잤어.\n",
      " 어제 치통 때문에 밤새 잠을 한숨도 못 잤어요.\n",
      "BLEU score: 0.462 0.167 0.000 0.000 0.000\n",
      "125/128 저런 빨리 병원 가다 진찰 보다 -> 저런 빨리 빨리 병원에 가야 해요.\n",
      " 저런.. 얼른 병원에 가서 진찰을 받아보세요.\n",
      "BLEU score: 0.285 0.128 0.073 0.000 0.000\n",
      "126/128 무엇 보다 건강 가장 중요 . -> 무엇을 보는 건강이 가장 중요해요?\n",
      " 무엇보다 건강이 제일 중요합니다.\n",
      "BLEU score: 0.400 0.111 0.000 0.000 0.000\n",
      "127/128 밥 약속어기다 하지마 꼭 꼭 밥 적당히 운동 건강 도움받다 . -> 밥 약속어기라고 하지마 꼭꼭 밥으로 건강을 도와주세요.\n",
      " 식사를 거르지 말고 꼬박꼬박 챙겨먹고 적당한 운동을 하면 건강에 도움이 될 거 에요.\n",
      "BLEU score: 0.190 0.000 0.000 0.000 0.000\n",
      "BLEU score: 0.389 0.139 0.071 0.036 0.050\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab.Tagger('-Owakati')\n",
    "\n",
    "df = pd.read_csv(\"../MY_DATA/gloss_from_book.csv\")\n",
    "SCORE1, SCORE2, SCORE3, SCORE4, SCORET = [], [], [], [], []\n",
    "for i in range(len(df)):\n",
    "    input_text = df.iloc[i]['gloss']\n",
    "    gt_text = df.iloc[i]['spoken']\n",
    "    output_text = generate_text(nlg, input_text, 'spoken', num_return_sequences=1, max_length=1000)[0]\n",
    "    gtext = mecab.parse(gt_text).split()\n",
    "    outputtext = mecab.parse(output_text).split()\n",
    "    score1 = sentence_bleu([gtext], outputtext, weights=(1, 0, 0, 0))\n",
    "    score2 = sentence_bleu([gtext], outputtext, weights=(0, 1, 0, 0))\n",
    "    score3 = sentence_bleu([gtext], outputtext, weights=(0, 0, 1, 0))\n",
    "    score4 = sentence_bleu([gtext], outputtext, weights=(0, 0, 0, 1))\n",
    "    scoret = sentence_bleu([gtext], outputtext)\n",
    "    print(f'{i}/{len(df)} {input_text} -> {output_text}')\n",
    "    print(gt_text)\n",
    "    print(f\"BLEU score: {score1:.3f} {score2:.3f} {score3:.3f} {score4:.3f} {scoret:.3f}\")\n",
    "    SCORE1.append(score1)\n",
    "    SCORE2.append(score2)\n",
    "    SCORE3.append(score3)\n",
    "    SCORE4.append(score4)\n",
    "    SCORET.append(scoret)\n",
    "BLEU1_AVG, BLEU2_AVG, BLEU3_AVG, BLEU4_AVG, BLEUT_AVG = np.mean(SCORE1), np.mean(SCORE2), np.mean(SCORE3), np.mean(SCORE4), np.mean(SCORET)\n",
    "print(f\"BLEU score: {BLEU1_AVG:.3f} {BLEU2_AVG:.3f} {BLEU3_AVG:.3f} {BLEU4_AVG:.3f} {BLEUT_AVG:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:04:27.602826771Z",
     "start_time": "2023-10-28T07:04:07.987914021Z"
    }
   },
   "id": "abb7a1118cb4db7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BLEU score: 0.387 0.158 0.084 0.039 0.056\n",
    "BLEU score: 0.394 0.148 0.077 0.040 0.060\n",
    "BLEU score: 0.389 0.139 0.071 0.036 0.050\n",
    "\n",
    "BLEU score: 0.418 0.089 0.036 0.015 0.028"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b8b23351ed75ab"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/128 안녕 ? 만나다 반갑다 . 나 농인 . ->  안녕하십니까? 만나서 반갑습니다. 저는 농인입니다.\n",
      "BLEU score: 0.500 0.077 0.000 0.000 0.000\n",
      "1/128 안녕 ? 만나다 반갑다 . 나 청인 . ->  안녕하십니까? 만나서 반갑습니다. 저는 청인입니다.\n",
      "BLEU score: 0.500 0.077 0.000 0.000 0.000\n",
      "2/128 당신 이름 무엇 ? ->  당신의 이름은 무엇입니까?\n",
      "BLEU score: 0.571 0.000 0.000 0.000 0.000\n",
      "3/128 당신 농인 ? ->  당신은 농인입니까?\n",
      "BLEU score: 0.600 0.000 0.000 0.000 0.000\n",
      "4/128 아니오 . 나 청인 . ->  아니오. 나는 청인입니다.\n",
      "BLEU score: 0.714 0.333 0.200 0.000 0.000\n",
      "5/128 고맙다 . 미안하다 . 괜찮다 . ->  고맙습니다. 미안합니다. 괜찮습니다.\n",
      "BLEU score: 0.597 0.224 0.000 0.000 0.000\n",
      "6/128 수고 다음 또 만나다 ->  수고하셨습니다. 다음에 또 뵙겠습니다.\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "7/128 나이 몇 ? 나 20 . ->  나이가 어떻게 되세요? 나는 20살입니다.\n",
      "BLEU score: 0.417 0.091 0.000 0.000 0.000\n",
      "8/128 나 같다 . 나 20 . ->  저랑 동갑이군요. 나도 20살입니다.\n",
      "BLEU score: 0.333 0.091 0.000 0.000 0.000\n",
      "9/128 우리 둘 친하게 지내다 괜찮다 . ->  우리 친하게 지냅시다.\n",
      "BLEU score: 0.359 0.112 0.000 0.000 0.000\n",
      "10/128 괜찮다 OK 좋다 . ->  좋아요.\n",
      "BLEU score: 0.245 0.000 0.000 0.000 0.000\n",
      "11/128 휴대폰 번호 무엇 ? ->  휴대폰 번호가 어떻게 되나요?\n",
      "BLEU score: 0.429 0.167 0.000 0.000 0.000\n",
      "12/128 나 휴대폰 번호 010-2456-2642 . ->  내 휴대폰 번호는 010-2456-2642입니다.\n",
      "BLEU score: 0.727 0.500 0.333 0.250 0.417\n",
      "13/128 앞으로 자주 문자메시지를 주고받다 연락하다 OK . ->  앞으로 자주 연락 나눠요.\n",
      "BLEU score: 0.260 0.125 0.078 0.000 0.000\n",
      "14/128 당신 1살 위 . ->  저보다 한 살 더 많으시네요.\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "15/128 나 2살 아래 . ->  제가 두 살 더 적습니다.\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "16/128 그 신발 돈 얼마 ? ->  그 신발은 얼마입니까?\n",
      "BLEU score: 0.667 0.200 0.000 0.000 0.000\n",
      "17/128 4만 5천 . ->  4만 5천원입니다.\n",
      "BLEU score: 0.714 0.500 0.400 0.250 0.435\n",
      "18/128 같다 전공 친구 . ->  같은 과 친구예요.\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "19/128 당신 전공 무엇 ? ->  전공이 무엇인가요?\n",
      "BLEU score: 0.600 0.000 0.000 0.000 0.000\n",
      "20/128 나 전공 무엇 수화 통역 . ->  나는 수화통역을 전공하고 있습니다.\n",
      "BLEU score: 0.455 0.100 0.000 0.000 0.000\n",
      "21/128 가족 소개 주세요 . ->  가족 소개를 해 주세요.\n",
      "BLEU score: 0.714 0.500 0.200 0.000 0.000\n",
      "22/128 나 가족 부모님 오빠 나 4 사람 . ->  우리 가족은 부모님 오빠 나 모두 4명입니다.\n",
      "BLEU score: 0.583 0.273 0.200 0.111 0.244\n",
      "23/128 나 가족 조부모님 부모님 누나 나 남동생 7사람 . ->  우리 가족은 할아버지 할머니 부모님 누나 나 남동생 모두 7명입니다.\n",
      "BLEU score: 0.533 0.286 0.231 0.167 0.277\n",
      "24/128 가족 많다 부럽다 . ->  가족이 많아 부럽군요.\n",
      "BLEU score: 0.571 0.000 0.000 0.000 0.000\n",
      "25/128 나 전공 사회 행복 . ->  내 전공은 사회복지입니다.\n",
      "BLEU score: 0.429 0.000 0.000 0.000 0.000\n",
      "26/128 나 특별 교육 공부 . ->  내 전공은 특수교육입니다.\n",
      "BLEU score: 0.286 0.000 0.000 0.000 0.000\n",
      "27/128 저 사람 같다 전공 선배 . ->  저분은 같은 과 선배입니다.\n",
      "BLEU score: 0.444 0.000 0.000 0.000 0.000\n",
      "28/128 저 사람 동아리 동기 . ->  저 사람은 동아리 동기입니다.\n",
      "BLEU score: 0.714 0.333 0.000 0.000 0.000\n",
      "29/128 당신 결혼 끝 ? ->  당신은 결혼을 했습니까?\n",
      "BLEU score: 0.429 0.000 0.000 0.000 0.000\n",
      "30/128 아니오 . 결혼 아직 . ->  아니오. 아직 안 했습니다.\n",
      "BLEU score: 0.571 0.167 0.000 0.000 0.000\n",
      "31/128 나 태어나다 곳 부산 . ->  내 고향은 부산입니다.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "32/128 부모님 태어나다 곳 부산 같다 ? ->  부모님 고향도 부산입니까?\n",
      "BLEU score: 0.429 0.125 0.000 0.000 0.000\n",
      "33/128 아니오 . 아버지 대구 어머니 부산 . ->  아니오. 아버지 고향은 대구이고 어머니 고향은 부산입니다.\n",
      "BLEU score: 0.500 0.154 0.083 0.000 0.000\n",
      "34/128 나 태어나다 곳 천안 . ->  내 고향은 천안입니다.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "35/128 나 태어나다 곳 천안 호두 과자 유명하다 . ->  내 고향 천안은 호두과자가 유명합니다.\n",
      "BLEU score: 0.267 0.000 0.000 0.000 0.000\n",
      "36/128 춘천 닭 갈비 유명하다 전주 비빔밥 유명하다 . ->  춘천에는 닭갈비가 유명하고 전주는 비빔밥이 우명합니다.\n",
      "BLEU score: 0.400 0.071 0.000 0.000 0.000\n",
      "37/128 우리 둘 함께 전국 맛있다 음식 찾다 여행 출발 어때 ? ->  우리 함께 전국의 맛있는 음식을 찾아 여행을 떠날까요?\n",
      "BLEU score: 0.533 0.071 0.000 0.000 0.000\n",
      "38/128 보성 방문 경험 ? ->  보성에 가본 적 있나요?\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "39/128 녹차밭 예쁘다 . ->  녹차 밭이 무척 예뻤어요.\n",
      "BLEU score: 0.429 0.167 0.000 0.000 0.000\n",
      "40/128 당신 미국 방문 경험 ? ->  미국에 가본 적 있어요?\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "41/128 그러나 1 방문 원하다 . ->  하지만 꼭 한번 가보고 싶어요.\n",
      "BLEU score: 0.111 0.000 0.000 0.000 0.000\n",
      "42/128 좋다 음식 무엇 ? ->  좋아하는 음식이 무엇인가요?\n",
      "BLEU score: 0.444 0.000 0.000 0.000 0.000\n",
      "43/128 나 비빔밥 좋다 . ->  나는 비빔밥을 좋아해요.\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "44/128 비빔밥 맛있다 이루다 장소 알다 ? ->  비빔밥 맛있는 음식점을 알고 있나요?\n",
      "BLEU score: 0.400 0.111 0.000 0.000 0.000\n",
      "45/128 먹다 장소 유명하다 . ->  식당이 유명해요.\n",
      "BLEU score: 0.268 0.000 0.000 0.000 0.000\n",
      "46/128 당신 취미 무엇 ? ->  취미가 뭐예요?\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "47/128 나 취미 무엇 영화 보다 . ->  내 취미는 영화감상이에요.\n",
      "BLEU score: 0.375 0.000 0.000 0.000 0.000\n",
      "48/128 요즈음 재미있다 영화 무엇 ? ->  요즈음 재미있는 영화가 있나요?\n",
      "BLEU score: 0.500 0.143 0.000 0.000 0.000\n",
      "49/128 그러나 한국 영화 자막 없다 섭섭하다 . ->  하지만 한국 영화는 자막이 없어서 안타까워요.\n",
      "BLEU score: 0.455 0.100 0.000 0.000 0.000\n",
      "50/128 배고프다 . 밥 출발 . ->  배고파요. 밥 먹으러 갑시다.\n",
      "BLEU score: 0.429 0.167 0.000 0.000 0.000\n",
      "51/128 나 같다 . 먹다 무엇 ? ->  저도요. 뭐 먹으러 갈까요?\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "52/128 맵다 김치 끓이다 괜찮다 ? ->  얼큰한 김치찌개 어때요?\n",
      "BLEU score: 0.110 0.000 0.000 0.000 0.000\n",
      "53/128 나 맵다 싫다 . ->  나는 매운 음식을 싫어해요.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "54/128 좋아하다 운동 무엇 ? ->  좋아하는 운동이 뭐예요?\n",
      "BLEU score: 0.556 0.250 0.143 0.000 0.000\n",
      "55/128 수영 탁구 좋아하다 . ->  나는 수영과 탁구를 좋아해요.\n",
      "BLEU score: 0.500 0.111 0.000 0.000 0.000\n",
      "56/128 당신 취미 무엇 ? ->  취미가 무엇인가요?\n",
      "BLEU score: 0.600 0.000 0.000 0.000 0.000\n",
      "57/128 나 취미 독서 음악 듣다 . ->  내 취미는 독서와 음악감상이에요.\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "58/128 4올 20일 날 무엇 ? ->  4월 20일이 무슨 날인가요?\n",
      "BLEU score: 0.556 0.125 0.000 0.000 0.000\n",
      "59/128 장애인 날 . ->  장애인의 날이에요.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "60/128 그렇군요 ! 농인 날 언제 ? ->  그렇군요! 농인의 날은 언제인가요?\n",
      "BLEU score: 0.700 0.333 0.250 0.143 0.302\n",
      "61/128 6월 3일 . ->  6월 3일이에요.\n",
      "BLEU score: 0.714 0.500 0.400 0.250 0.435\n",
      "62/128 10시 30분 . ->  10시 30분이에요.\n",
      "BLEU score: 0.714 0.500 0.400 0.250 0.435\n",
      "63/128 11시 공부 시작 . 나 먼저 가다 . ->  11시에 수업이 시작합니다. 나 먼저 가볼게요.\n",
      "BLEU score: 0.615 0.333 0.182 0.100 0.247\n",
      "64/128 알다 . 다음 또 만나다 . ->  알았어요. 다음에 또 만나요.\n",
      "BLEU score: 0.600 0.222 0.000 0.000 0.000\n",
      "65/128 당신 태어나다 날 언제 ? ->  생일이 언제인가요?\n",
      "BLEU score: 0.327 0.000 0.000 0.000 0.000\n",
      "66/128 8월 18일 . ->  8월 18일이에요.\n",
      "BLEU score: 0.714 0.500 0.400 0.250 0.435\n",
      "67/128 5월 15일 날 무엇 ? ->  5월 15일은 무슨 날인가요?\n",
      "BLEU score: 0.667 0.375 0.286 0.167 0.330\n",
      "68/128 교사 날 . ->  스승의 날이에요.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "69/128 7시 시작 . ->  7시에 시작합니다.\n",
      "BLEU score: 0.667 0.200 0.000 0.000 0.000\n",
      "70/128 공부 시작 9시 끝 12시 . ->  수업은9시에 시작해서 12시에 끝납니다.\n",
      "BLEU score: 0.500 0.182 0.000 0.000 0.000\n",
      "71/128 오늘 봄 비 . ->  오늘은 봄비가 내리고 있어요.\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "72/128 비 멈추다 날씨 따뜻하다 아마 . ->  비가 그치면 날씨가 따뜻해질 겁니다.\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "73/128 며칠 기다리다 꽃피다 오다 아마 ? ->  며칠 있으면 꽃이 피겠군요?\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "74/128 나 봄 노랗다 꽃 좋아하다 . ->  나는 봄에 피는 개나리를 좋아합니다.\n",
      "BLEU score: 0.417 0.091 0.000 0.000 0.000\n",
      "75/128 좋다 계절 무엇 ? ->  어떤 계절을 좋아하나요?\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "76/128 나 가을 좋다 하늘 깨끗하다 빨갛다 잎 멋있다 . ->  맑은 하늘과 단풍이 멋진 가을이 좋습니다.\n",
      "BLEU score: 0.282 0.000 0.000 0.000 0.000\n",
      "77/128 나 눈 겨울 좋다 . ->  나는 눈 내리는 겨울이 좋습니다.\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "78/128 한국 계절 각각 멋있다 모두 좋다 . ->  우리나라의 사철은 나름대로 운치 있어 모두 좋습니다.\n",
      "BLEU score: 0.214 0.077 0.000 0.000 0.000\n",
      "79/128 하늘 구름 비 아마 . ->  하늘이 흐린 걸 보니 비가 올 모양이에요.\n",
      "BLEU score: 0.231 0.000 0.000 0.000 0.000\n",
      "80/128 우산 미리 준비 부탁 . ->  우산을 미리 준비하세요.\n",
      "BLEU score: 0.571 0.167 0.000 0.000 0.000\n",
      "81/128 날씨 춥다 옷 옷이 두껍다 입다 부탁 . ->  날씨가 추우니까 옷을 두둑하게 입으세요.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "82/128 가다 어느 곳 ? ->  지금 어디에 가요?\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "83/128 책 읽다 집 가다 중 . ->  도서관에 가고 있어요.\n",
      "BLEU score: 0.143 0.000 0.000 0.000 0.000\n",
      "84/128 책 읽다 집 가다 왜 ? ->  도서관에는 무슨 일로 가나요?\n",
      "BLEU score: 0.222 0.000 0.000 0.000 0.000\n",
      "85/128 다음주 시험 때문에 공부 위하여 . ->  다음 주가 시험이라 공부하러 갑니다.\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "86/128 당신 일 무엇 ? ->  무슨 일 하세요?\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "87/128 나 농인 협회 수화 통역 사람 . ->  농아인협회에서 수화통역사로 일합니다.\n",
      "BLEU score: 0.300 0.000 0.000 0.000 0.000\n",
      "88/128 수화 통역 사람 일 무엇 ? ->  수화통역사는 무슨 일을 하나요?\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "89/128 농인 청인 대화 돕다 사람 . ->  농인과 청인의 의사소통을 지원합니다.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "90/128 숙제 끝 ? ->  과제 다 했니?\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "91/128 숙제 무엇 ? 아이쿠 잊다 . ->  과제라니? 아이쿠. 깜빡 잊고 있었어.\n",
      "BLEU score: 0.333 0.091 0.000 0.000 0.000\n",
      "92/128 미래 원하다 희망 무엇 ? ->  장래희망이 무엇인가요?\n",
      "BLEU score: 0.423 0.000 0.000 0.000 0.000\n",
      "93/128 나 유치원 교사 원하다 . ->  나는 유치원선생님이 되고 싶어요.\n",
      "BLEU score: 0.273 0.000 0.000 0.000 0.000\n",
      "94/128 나 아기 좋아하다 . ->  어린이들을 좋아하거든요.\n",
      "BLEU score: 0.141 0.000 0.000 0.000 0.000\n",
      "95/128 당신 잘 어울리다 . ->  당신에게 잘 어울리는 거 같아요.\n",
      "BLEU score: 0.444 0.125 0.000 0.000 0.000\n",
      "96/128 내일 여자 친구 생일 . ->  내일이 여자친구 생일이에요.\n",
      "BLEU score: 0.625 0.286 0.167 0.000 0.000\n",
      "97/128 선물 준비 끝 ? ->  선물은 준비했나요?\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "98/128 준비 아직 . 좋은 선물 무엇 ? ->  아직 준비를 못했어요. 뭐가 좋을까요?\n",
      "BLEU score: 0.417 0.000 0.000 0.000 0.000\n",
      "99/128 꽃 또 화장품 어때 ? ->  꽃과 화장품은 어때요?\n",
      "BLEU score: 0.500 0.000 0.000 0.000 0.000\n",
      "100/128 동생 살다 잘 ? ->  동생은 잘 지내고 있나요?\n",
      "BLEU score: 0.375 0.000 0.000 0.000 0.000\n",
      "101/128 진동 시계 선물 주다 해보다 ? ->  진동알람시계를 선물해 주는 건 어떄요?\n",
      "BLEU score: 0.417 0.000 0.000 0.000 0.000\n",
      "102/128 원하다 물건 무엇 ? ->  찾으시는 물건이 있나요?\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "103/128 우선 구경 보다 . ->  우선 둘러볼게요.\n",
      "BLEU score: 0.389 0.000 0.000 0.000 0.000\n",
      "104/128 이건 구두 마음에 들다 . 270 주세요. ->  이 구두가 마음에 들어요. 270사이즈 주세요.\n",
      "BLEU score: 0.692 0.417 0.182 0.000 0.000\n",
      "105/128 미안 270 바닥나다 ? ->  죄송합니다. 270사이즈는 다 팔리고 없습니다.\n",
      "BLEU score: 0.167 0.000 0.000 0.000 0.000\n",
      "106/128 천안 부터 대전 까지 가장 빠르다 방법 무엇 ? ->  천안에서 대전까지 가는 가장 빠른 방법은 무엇인가요?\n",
      "BLEU score: 0.538 0.083 0.000 0.000 0.000\n",
      "107/128 KTX 비싸 ? ->  KTX는 비싸지 않나요?\n",
      "BLEU score: 0.429 0.000 0.000 0.000 0.000\n",
      "108/128 농인 50% 할인 도움 받다 조금 괜찮다 . ->  농인은 50% 할인이 되기 때문에 부담이 덜합니다.\n",
      "BLEU score: 0.333 0.143 0.077 0.000 0.000\n",
      "109/128 여름 방학 계획 무엇 ? ->  이번 여름방학 계획이 어떻게 되나요?\n",
      "BLEU score: 0.444 0.250 0.143 0.000 0.000\n",
      "110/128 친구 함께 자전거 여행 결정 . ->  친구들과 함께 자전거 여행을 하기로 했어요.\n",
      "BLEU score: 0.385 0.167 0.091 0.000 0.000\n",
      "111/128 여행 목적 어느 곳 ? ->  어디로 가나요?\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "112/128 서울 출발 부산 까지 . ->  서울에서 출발하여 부산까지 갑니다.\n",
      "BLEU score: 0.556 0.125 0.000 0.000 0.000\n",
      "113/128 운전 가능 ? ->  운전할 수 있어요?\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "114/128 아직 배우다 중 그러나 주차 가장 어렵다 . ->  아니요. 지금 배우고 있어요. 그런데 주차가 가장 어려운 것 같아요.\n",
      "BLEU score: 0.235 0.000 0.000 0.000 0.000\n",
      "115/128 나 같다 ->  나도 그랬어요.\n",
      "BLEU score: 0.200 0.000 0.000 0.000 0.000\n",
      "116/128 합격 꼭 화이팅 . ->  꼭 합격하시길 바랍니다.\n",
      "BLEU score: 0.429 0.000 0.000 0.000 0.000\n",
      "117/128 소화 안돼 머리 아프다 . ->  소화가 잘 안 되고 머리가 아파요.\n",
      "BLEU score: 0.400 0.000 0.000 0.000 0.000\n",
      "118/128 병원 가다 오다 끝 ? ->  병원에는 다녀왔어요?\n",
      "BLEU score: 0.282 0.000 0.000 0.000 0.000\n",
      "119/128 2시 수화 통역 사람 만나다 약속 . ->  2시에 수화통역사와 만나기로 했어요.\n",
      "BLEU score: 0.417 0.091 0.000 0.000 0.000\n",
      "120/128 오늘 공부 쉬다 . ->  이번 수업 휴강이래요.\n",
      "BLEU score: 0.167 0.000 0.000 0.000 0.000\n",
      "121/128 왜 ? ->  왜요 ?\n",
      "BLEU score: 0.667 0.000 0.000 0.000 0.000\n",
      "122/128 교사 맹장 수술 때문에 입원 중 . ->  교수님께서 맹장수술을 하고 입원해 계신대요.\n",
      "BLEU score: 0.333 0.091 0.000 0.000 0.000\n",
      "123/128 함께 병 위로 출발 . ->  우리 함께 병문안 갑시다.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "124/128 어제 치아 아프다 데굴데굴 구르다 잠 못 이루다 . ->  어제 치통 때문에 밤새 잠을 한숨도 못 잤어요.\n",
      "BLEU score: 0.308 0.000 0.000 0.000 0.000\n",
      "125/128 저런 빨리 병원 가다 진찰 보다 ->  저런.. 얼른 병원에 가서 진찰을 받아보세요.\n",
      "BLEU score: 0.333 0.000 0.000 0.000 0.000\n",
      "126/128 무엇 보다 건강 가장 중요 . ->  무엇보다 건강이 제일 중요합니다.\n",
      "BLEU score: 0.625 0.286 0.167 0.000 0.000\n",
      "127/128 밥 약속어기다 하지마 꼭 꼭 밥 적당히 운동 건강 도움받다 . ->  식사를 거르지 말고 꼬박꼬박 챙겨먹고 적당한 운동을 하면 건강에 도움이 될 거 에요.\n",
      "BLEU score: 0.250 0.000 0.000 0.000 0.000\n",
      "BLEU score: 0.418 0.089 0.036 0.015 0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "mecab = Mecab.Tagger('-Owakati')\n",
    "\n",
    "df = pd.read_csv(\"../MY_DATA/gloss_from_book.csv\")\n",
    "SCORE1, SCORE2, SCORE3, SCORE4, SCORET = [], [], [], [], []\n",
    "for i in range(len(df)):\n",
    "    input_text = df.iloc[i]['gloss']\n",
    "    gt_text = df.iloc[i]['spoken']\n",
    "    gtext = mecab.parse(input_text).split()\n",
    "    outputtext = mecab.parse(gt_text).split()\n",
    "    score1 = sentence_bleu([gtext], outputtext, weights=(1, 0, 0, 0))\n",
    "    score2 = sentence_bleu([gtext], outputtext, weights=(0, 1, 0, 0))\n",
    "    score3 = sentence_bleu([gtext], outputtext, weights=(0, 0, 1, 0))\n",
    "    score4 = sentence_bleu([gtext], outputtext, weights=(0, 0, 0, 1))\n",
    "    scoret = sentence_bleu([gtext], outputtext)\n",
    "    print(f'{i}/{len(df)} {input_text} -> {gt_text}')\n",
    "    print(f\"BLEU score: {score1:.3f} {score2:.3f} {score3:.3f} {score4:.3f} {scoret:.3f}\")\n",
    "    SCORE1.append(score1)\n",
    "    SCORE2.append(score2)\n",
    "    SCORE3.append(score3)\n",
    "    SCORE4.append(score4)\n",
    "    SCORET.append(scoret)\n",
    "BLEU1_AVG, BLEU2_AVG, BLEU3_AVG, BLEU4_AVG, BLEUT_AVG = np.mean(SCORE1), np.mean(SCORE2), np.mean(SCORE3), np.mean(SCORE4), np.mean(SCORET)\n",
    "print(f\"BLEU score: {BLEU1_AVG:.3f} {BLEU2_AVG:.3f} {BLEU3_AVG:.3f} {BLEU4_AVG:.3f} {BLEUT_AVG:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-28T07:12:20.692685342Z",
     "start_time": "2023-10-28T07:12:20.628435236Z"
    }
   },
   "id": "e267d9ec7c2e3249"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43055451af3de6b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
