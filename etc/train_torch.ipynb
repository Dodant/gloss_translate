{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:45:04.225293775Z",
     "start_time": "2023-09-26T06:45:04.180487470Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1+cu102\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:45:04.540963533Z",
     "start_time": "2023-09-26T06:45:04.538275182Z"
    }
   },
   "id": "f07ea540720a5653"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Gloss Translator based on KoGPT-2')\n",
    "parser.add_argument('--chat', action='store_true', default=False, help='translation on given user input')\n",
    "parser.add_argument('--sentence', type=str, default='0', help='0 is declarative, 1 is interrogative, 2 is exclamatory')\n",
    "parser.add_argument('--model_params', type=str, default='model_chp/model_last.ckpt', help='model binary for translation')\n",
    "parser.add_argument('--train', action='store_true', default=False, help='training mode')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:45:08.204042929Z",
     "start_time": "2023-09-26T06:45:08.197792027Z"
    }
   },
   "id": "97c690788e83a215"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/2.69M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d732dc73792a4f70888e1978bbeb5a72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a390296992d4d80bc364e66cea3ce41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "U_TKN = '<usr>'\n",
    "S_TKN = '<sys>'\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "SENT = '<unused1>'\n",
    "PAD = '<pad>'\n",
    "\n",
    "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "                                                    bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "                                                    pad_token=PAD, mask_token=MASK) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:45:12.436661869Z",
     "start_time": "2023-09-26T06:45:08.744851354Z"
    }
   },
   "id": "eb4a2919af7f6f7f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=32):\n",
    "        self._data = chats\n",
    "        self.first = True\n",
    "        self.q_token = U_TKN\n",
    "        self.a_token = S_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.bos = BOS\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.pad = PAD\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = TOKENIZER\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn['gloss']\n",
    "        a = turn['spoken']\n",
    "        sentiment = str(turn['label'])\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token + sentiment)\n",
    "        q_len = len(q_toked)\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len\n",
    "            if a_len <= 0:\n",
    "                q_toked = q_toked[-(int(self.max_len/2)):]\n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len\n",
    "                assert a_len > 0\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
    "        # [mask, mask, ...., mask, ..., <bos>,..A.. <eos>, <pad>....]\n",
    "        labels = [\n",
    "                     self.mask,\n",
    "                 ] * q_len + a_toked[1:]\n",
    "        \n",
    "        if self.first:\n",
    "            logging.info(\"contexts : {}\".format(q))\n",
    "            logging.info(\"toked ctx: {}\".format(q_toked))\n",
    "            logging.info(\"response : {}\".format(a))\n",
    "            logging.info(\"toked response : {}\".format(a_toked))\n",
    "            logging.info('labels {}'.format(labels))\n",
    "            self.first = False\n",
    "            \n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        self.max_len\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "        return(token_ids, np.array(mask),\n",
    "               labels_ids)\n",
    "\n",
    "\n",
    "class KoGPT2Chat(LightningModule):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super(KoGPT2Chat, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.neg = -1e18\n",
    "        self.kogpt2 = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        # add model specific args\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--max-len',\n",
    "                            type=int,\n",
    "                            default=32,\n",
    "                            help='max sentence length on input (default: 32)')\n",
    "\n",
    "        parser.add_argument('--batch-size',\n",
    "                            type=int,\n",
    "                            default=96,\n",
    "                            help='batch size for training (default: 96)')\n",
    "        parser.add_argument('--lr',\n",
    "                            type=float,\n",
    "                            default=5e-5,\n",
    "                            help='The initial learning rate')\n",
    "        parser.add_argument('--warmup_ratio',\n",
    "                            type=float,\n",
    "                            default=0.1,\n",
    "                            help='warmup ratio')\n",
    "        return parser\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (batch, seq_len, hiddens)\n",
    "        output = self.kogpt2(inputs, return_dict=True)\n",
    "        return output.logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        token_ids, mask, label = batch\n",
    "        out = self(token_ids)\n",
    "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "        mask_out = torch.where(mask_3d == 1, out, self.neg * torch.ones_like(out))\n",
    "        loss = self.loss_function(mask_out.transpose(2, 1), label)\n",
    "        loss_avg = loss.sum() / mask.sum()\n",
    "        self.log('train_loss', loss_avg)\n",
    "        return loss_avg\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.lr, correct_bias=False)\n",
    "        # warm up lr\n",
    "        num_train_steps = len(self.train_dataloader()) * self.hparams.max_epochs\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        lr_scheduler = {'scheduler': scheduler, 'name': 'cosine_schedule_with_warmup',\n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        data = [item[0] for item in batch]\n",
    "        mask = [item[1] for item in batch]\n",
    "        label = [item[2] for item in batch]\n",
    "        return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        data = pd.read_csv('MY_DATA/mydata_edited.csv')\n",
    "        self.train_set = CharDataset(data, max_len=self.hparams.max_len)\n",
    "        train_dataloader = DataLoader(\n",
    "            self.train_set, batch_size=self.hparams.batch_size, num_workers=2,\n",
    "            shuffle=True, collate_fn=self._collate_fn)\n",
    "        return train_dataloader\n",
    "\n",
    "    def chat(self, sent='0'):\n",
    "        tok = TOKENIZER\n",
    "        sent_tokens = tok.tokenize(sent)\n",
    "        with torch.no_grad():\n",
    "            while 1:\n",
    "                q = input('gloss > ').strip()\n",
    "                if q == 'quit':\n",
    "                    break\n",
    "                a = ''\n",
    "                while 1:\n",
    "                    input_ids = torch.LongTensor(tok.encode(U_TKN + q + SENT + sent + S_TKN + a)).unsqueeze(dim=0)\n",
    "                    pred = self(input_ids)\n",
    "                    gen = tok.convert_ids_to_tokens(\n",
    "                        torch.argmax(\n",
    "                            pred,\n",
    "                            dim=-1).squeeze().numpy().tolist())[-1]\n",
    "                    if gen == EOS:\n",
    "                        break\n",
    "                    a += gen.replace('â–', ' ')\n",
    "                print(\"translation > {}\".format(a.strip()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:49:51.526364630Z",
     "start_time": "2023-09-26T06:49:51.464972664Z"
    }
   },
   "id": "8766d52ae293d3a8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--chat] [--sentence SENTENCE]\n",
      "                             [--model_params MODEL_PARAMS] [--train]\n",
      "                             [--max-len MAX_LEN] [--batch-size BATCH_SIZE]\n",
      "                             [--lr LR] [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--logger [LOGGER]]\n",
      "                             [--checkpoint_callback [CHECKPOINT_CALLBACK]]\n",
      "                             [--enable_checkpointing [ENABLE_CHECKPOINTING]]\n",
      "                             [--default_root_dir DEFAULT_ROOT_DIR]\n",
      "                             [--gradient_clip_val GRADIENT_CLIP_VAL]\n",
      "                             [--gradient_clip_algorithm GRADIENT_CLIP_ALGORITHM]\n",
      "                             [--process_position PROCESS_POSITION]\n",
      "                             [--num_nodes NUM_NODES]\n",
      "                             [--num_processes NUM_PROCESSES]\n",
      "                             [--devices DEVICES] [--gpus GPUS]\n",
      "                             [--auto_select_gpus [AUTO_SELECT_GPUS]]\n",
      "                             [--tpu_cores TPU_CORES] [--ipus IPUS]\n",
      "                             [--log_gpu_memory LOG_GPU_MEMORY]\n",
      "                             [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE]\n",
      "                             [--enable_progress_bar [ENABLE_PROGRESS_BAR]]\n",
      "                             [--overfit_batches OVERFIT_BATCHES]\n",
      "                             [--track_grad_norm TRACK_GRAD_NORM]\n",
      "                             [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH]\n",
      "                             [--fast_dev_run [FAST_DEV_RUN]]\n",
      "                             [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]\n",
      "                             [--max_epochs MAX_EPOCHS]\n",
      "                             [--min_epochs MIN_EPOCHS] [--max_steps MAX_STEPS]\n",
      "                             [--min_steps MIN_STEPS] [--max_time MAX_TIME]\n",
      "                             [--limit_train_batches LIMIT_TRAIN_BATCHES]\n",
      "                             [--limit_val_batches LIMIT_VAL_BATCHES]\n",
      "                             [--limit_test_batches LIMIT_TEST_BATCHES]\n",
      "                             [--limit_predict_batches LIMIT_PREDICT_BATCHES]\n",
      "                             [--val_check_interval VAL_CHECK_INTERVAL]\n",
      "                             [--flush_logs_every_n_steps FLUSH_LOGS_EVERY_N_STEPS]\n",
      "                             [--log_every_n_steps LOG_EVERY_N_STEPS]\n",
      "                             [--accelerator ACCELERATOR] [--strategy STRATEGY]\n",
      "                             [--sync_batchnorm [SYNC_BATCHNORM]]\n",
      "                             [--precision PRECISION]\n",
      "                             [--enable_model_summary [ENABLE_MODEL_SUMMARY]]\n",
      "                             [--weights_summary WEIGHTS_SUMMARY]\n",
      "                             [--weights_save_path WEIGHTS_SAVE_PATH]\n",
      "                             [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--profiler PROFILER] [--benchmark [BENCHMARK]]\n",
      "                             [--deterministic [DETERMINISTIC]]\n",
      "                             [--reload_dataloaders_every_n_epochs RELOAD_DATALOADERS_EVERY_N_EPOCHS]\n",
      "                             [--reload_dataloaders_every_epoch [RELOAD_DATALOADERS_EVERY_EPOCH]]\n",
      "                             [--auto_lr_find [AUTO_LR_FIND]]\n",
      "                             [--replace_sampler_ddp [REPLACE_SAMPLER_DDP]]\n",
      "                             [--detect_anomaly [DETECT_ANOMALY]]\n",
      "                             [--auto_scale_batch_size [AUTO_SCALE_BATCH_SIZE]]\n",
      "                             [--prepare_data_per_node [PREPARE_DATA_PER_NODE]]\n",
      "                             [--plugins PLUGINS] [--amp_backend AMP_BACKEND]\n",
      "                             [--amp_level AMP_LEVEL]\n",
      "                             [--move_metrics_to_cpu [MOVE_METRICS_TO_CPU]]\n",
      "                             [--multiple_trainloader_mode MULTIPLE_TRAINLOADER_MODE]\n",
      "                             [--stochastic_weight_avg [STOCHASTIC_WEIGHT_AVG]]\n",
      "                             [--terminate_on_nan [TERMINATE_ON_NAN]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/dodant/.local/share/jupyter/runtime/kernel-14f18e7d-0235-4ccc-b262-b168dc4ef65e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dodant/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = KoGPT2Chat.add_model_specific_args(parser)\n",
    "parser = Trainer.add_argparse_args(parser)\n",
    "args = parser.parse_args()\n",
    "logging.info(args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T06:50:04.789272605Z",
     "start_time": "2023-09-26T06:50:04.773193383Z"
    }
   },
   "id": "bfbd5d60856b8c87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9786db050035d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
